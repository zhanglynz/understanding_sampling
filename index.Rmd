--- 
title: "Understanding Survey Sampling"
author: "Lingyun Zhang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
link-citations: yes
links-as-notes: true
colorlinks: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preface {-}

How do you learn, or to be exact, how do you learn complex stuff? I guess that many different answers will come out, because we are all different in many aspects, such as culture background, knowledge level, and learning style etc. etc.. For myself, if I want to seriously learn some complex stuff, then I will write about it. Without doubts, *Sampling* is complex, and it is kind of boring, but it is important (for me, because I need it for my work, and for you the reasons must be different). So, I have to learn and re-learn sampling, and now I am writing about it.

Since I live and work in New Zealand, I will use contexts and examples at there. Chapter 1 introduces preliminary concepts, such as *population*, *target population*, *survey populaton*, *simple random sample*, *stratified sample*, *cluster sample*, and *systematic sample*. Chapter 1 also presents some knowledge about New Zealand. Chapter 2 is about *sampling design*; Chapter 3 is about *weighting system*; Chapter 4 covers how to estimate *sampling errors*. Chapter 5 briefly introduces software for sampling.

# Introduction {-}

*Sampling* is all about getting/taking a **sample** from a **population**. It is clear that a sample is just a subset of a population. At the end of day, what we are concerned about are data---population and sample both are data and they can be represented as tables. So, for a population, we can use the following table to represent/show it.

 Unit_ID | $Y_1$ | $Y_2$ | $\cdots$ | $Y_m$
:---------|:-------|:-------|:----------|:-------
 1        |        |        |           |
 2        |        |        |           |
 $\vdots$ |        |        |           |
 $N$      |        |        |           |

The population shown in the above table has size $N$, and we are interested in the $m$ **unknown** variables, $Y_1\, Y_2,\ \ldots,\ Y_m$. A part/portion of the above table is a sample. We make efforts to use the **known** sample (i.e. collected data) to get insights into the whole population---this is serious business. 

Sampling is challenging, because not only must we deal with mathematics but also we must cope with many practical issues, such as defining population, reducing cost (as much as we can), and non-response etc. etc.. Part I of this book is about theory, and Part II focuses on practical matters in sampling.

# (PART\*) Part I: Theory {-}

# Toy Examples

**Example 1:** **Simple Random Sampling**

The population is shown as the following table.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
a <- c(6, 8, 8, 1, 6, 4, 7, 5, 3, 3)
index_m <- combn(1:10, 5)
m <- dim(index_m)[2]
y_bar <- rep(0, m)
s2 <- rep(0, m)
for(i in 1:m) 
{y_bar[i] <- mean(a[index_m[, i]])
 s2[i] <- var(a[index_m[, i]]) 
}
mean_of_s2 <- mean(s2)
```

Unit_ID | $Y$
:-------| :------
  1| 6
  2| 8
  3| 8
  4| 1
  5| 6
  6| 4
  7| 7
  8| 5
  9| 3
 10| 3

The population size $N=10$. Let us choose the sample size $n$ be $5$.

- There are ${10 \choose 5}=252$ possible samples---here we are talking about *simple random samples* (SRS). In practice, we can only have one sample, and there will always be **uncertainty** if we use one sample to infer the whole population.
- The population mean $\bar{Y}=5.1$.
- The distribution of all possible sample means is shown below. The mean of the distribution is $5.1$, which is the same as the population mean $\bar{Y}$. So, sample mean is an **unbiased** estimator of the population mean. 
```{r, message=FALSE, warning=FALSE, echo=FALSE}
hist(y_bar)
```

- The population variance is
$$
S^2=\frac{\sum_{i=1}^N (Y_i - \bar{Y})^2}{N-1}=5.43
$$

- The distribution of all possible sample variances (NB: the divisor is $n-1$) is shown below. The mean of the distribution is `r round(mean_of_s2, 2)`, which is the same as the population variance. So, under *simple random sampling*, sample variance is an **unbiased** estimator of the population variance.  
```{r, message=FALSE, warning=FALSE, echo=FALSE}
hist(s2)
```

**Example 2:** **Systematic Sampling**

We use the same population as in Example 1, and we take 100,000 systematic samples of size 5. (NB: These samples are also simple random samples.)

```{r, warning=FALSE, message==FALSE, echo=TRUE}
a <- c(6, 8, 8, 1, 6, 4, 7, 5, 3, 3)
simu_run_nbr <- 100000

my_sys_sample <- matrix(0, 5, simu_run_nbr)

for(i in 1:simu_run_nbr)
{my_sys_sample[, i] <- sample(a)[1:5]
}

the_sample_mean <- apply(my_sys_sample, 2, mean)

hist(the_sample_mean, main = "Histogram of sample means", xlab = "y-bar")

(mean(the_sample_mean))

the_sample_var <- apply(my_sys_sample, 2, var)

hist(the_sample_var, main = "Histogram of sample variances", xlab = "s2")

(mean(the_sample_var))
```



# Sampling design

## General ideas

Throughout this chapter, *sampling design* means *probability sampling design*. Following (but not strictly) Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017), the settings are as follows:

- The *universe* is ${\cal U}=\{1, 2, \ldots, i, \ldots, N\}$.
- A *sample* $S$ is a subset of ${\cal U}$. We restrict that $S$ cannot be empty.
- There are $2^N-1$ possible samples. Let the set of all the possible samples be denoted by $\Omega$.

A sampling design specifies a probability distribution $p(\cdot)$ over $\Omega$ such that
$$
p(S)\ge 0\ \hbox{and}\ \sum_{S\in \Omega} p(S)=1.
$$

Define 
$$
\pi_i = \hbox{the probability of selecting unit}\ i
$$
and for $i\neq j$
$$
\pi_{ij} = \hbox{the probability that both units}\ i\ \hbox{and}\ j\ \hbox{are selected in the sample.}
$$
Then,
$$
\pi_i = \sum_{i \in S}p(S)\ \hbox{and}\ \pi_{ij}=\sum_{\{i,\ j\}\subset S}p(S).
$$

Examples of sampling designs:

- **simple random sampling:**
$$
p(S)=\left\{
\begin{array}{ll}
{N \choose n}^{-1}, & \hbox{if}\ S\in S_n,\\
0, & \hbox{otherwise},
\end{array}
\right.
$$
where $S_n= \{S\in \Omega|\#S = n\}$ and $n$ is the sample size. For this sampling design, 
$$
\pi_i = \frac{n}{N}\ \hbox{and}\ \pi_{ij}= \frac{n(n-1)}{N(N-1)}.
$$
- **Strafied sampling:**
$$
p(S)=\left\{
\begin{array}{ll}
\prod_{h=1}^H{N_h \choose n_h}^{-1}, &\hbox{if}\ \#(S\bigcap {\cal U}_h)=n_h\ \hbox{for}\ h=1,\ldots, H,\\
0, & \hbox{otherwise},
\end{array}
\right.
$$
where the universe ${\cal U}$ is partitioned into $H$ strata and $\# {\cal U}_h= N_h$ for $h=1, \ldots, H$; $n_h$ is the sample size for stratum ${\cal U}_h$. For this sampling design,
$$
\pi_i = \frac{n_h}{N_h},\ \hbox{if}\ i\in {\cal U}_h,
$$
and
$$
\pi_{ij}=\left\{
\begin{array}{ll}
\frac{n_h(n_h-1)}{N_h(N_h-1)}, & \hbox{if}\ i, j\in {\cal U}_h,\\
\frac{n_gn_h}{N_gN_h}, &\hbox{if}\ i \in {\cal U}_g,\ j\in {\cal U}_h,\ g\neq h.
\end{array}
\right.
$$

## Probability proportional to size sampling 

In a sampling design, an important part is *selection probabilities*, which accompany the items/individuals in the population. We can distinguish two cases. Case 1: all the selection probabilities are equal. Case 2: the selection probabilities are not equal. Simple random sampling is an example of Case 1; probability proportional to size (PPS) sampling is an example of Case 2.

Under PPS, the selection probability of item/individual $i$ is defined as
\begin{equation}
\pi_i = n \frac{Z_i}{\sum_{i=1}^N Z_i},\ \text{for}\ i=1, \ldots, N,
(\#eq:pi)
\end{equation}
where $n$ and $N$ are the sample size and population size, respectively, $Z_i$ is the *size/importance* of item/individual $i$.

It is easy to see from \@ref(eq:pi) that
\begin{equation}
\sum_{i=1}^N \pi_i = n.
(\#eq:pisum)
\end{equation}
Actually, \@ref(eq:pisum) is always hold if the sample size is fixed as $n$; below is a quick proof.

**Proof:** 
$$
\sum_{i=1}^N \pi_i= \frac{N\binom{N-1}{n-1}}{\binom{N}{n}}\times 1
=n.
$$

## Three theoretical principles

Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017)
introduce three theoretical principles for sampling design.

- **Randomization:** a) make sure there are as many samples as possible, while meeting other constraints; b) select a sample at random.

- **Overrepresentation:** unequal inclusion probabilities often result in more efficient estimates, or in other words, we should "preferentially select units where the dispersion is larger."

- **Restriction:** avoid bad samples, e.g. using auxiliary information to make sure the estimates from a sample approximately equal the known totals. That is, "samples that either nonpractical or known to be inaccurate are avoided."

We quote the following from Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017).

> "When auxiliary information is available, it is desired to include it in the sampling design in order to increase the precision of the estimates."
>

> "A balanced sample is such that the estimated totals of the auxiliary variables are approximately equal to the true totals."
>

# Weighting system

The main reference for this chapter is Haziza and Beaumont (2017).

## General ideas about weighting

Since a sample is a part/portion of a population, each item/individual in the sample represents itself plus others in the population; that is, a weight $w_i$ should be attached to the item/individual $i$. It's obvious that 

$$w_i>0$$
or sometimes
$$
w_i \ge 1.
$$

Weights are important because they are used in estimation of population parameters. Given the data, i.e. variable values and weights

$y$ | weight
:---:|:------:
$y_1$ | $w_1$
$y_2$ | $w_2$
$\vdots$ | $\vdots$
$y_n$ | $w_n$

we have estimates of the total and mean 
\begin{align}
\hat{t} &= \sum_{i=1}^n w_i y_i,\\
\hat{\bar{y}} &= \frac{\sum_{i=1}^n w_i y_i}{\sum_{i=1}^n w_i}.
\end{align}
In the literature, the estimator $\hat{t}$ is called the Horvitz-Thompson (HT) estimator.

## The three stages in producing final weights

- Stage 1: design weights
$$
d_i = \frac{1}{\pi_i},
$$
where $\pi_i$ is the selection/inclusion probability for item/individual $i$.

- Stage 2: weights adjusted for non-response. We use $\tilde{d}_i$ to denote the adjusted weight for item/individual $i$.

- Stage 3: final weights. We use $w_i$ to denote the final weight for item/individual $i$.

## Calibration

**Notation**

- We denote a population by $U$.
- A sample is denoted by $S$.
- Design weights: $\{d_k,\ k\in S\}$.
- Final weights: $\{w_k,\ k \in S\}$.

**What does calibration mean?**

Roughly speaking, calibration is to adjust from the design weights $\{d_i\}$ to final weights $\{w_i\}$ such that

a. $\{w_i\}$ are as close to $\{d_i\}$ as possible;
b. $\{w_i\}$ satisfy calibration constraints.

We will explain the exact meanings of a. and b. shortly.

**Why do we do calibration?**
 
According to Haziza and Beaumont (2017):

> The reasons for using calibration are three-fold:

> (i) to force consistency of certain survey estimates to known population quantities;  

> (ii) to reduce nonsampling errors such as nonresponse errors and coverage errors;

> (iii) to improve the precision of estimates.
>

**How do we do calibration?**

It's clear that calibration is an optimization problem, where the *decision variables* are $\{w_i\}$. The *objective function* is 
$$
\sum_{k\in S} d_k\frac{G(w_k/d_k)}{q_k},
$$
where $G(\cdot)$ is referred as a *distance function*---it measures the distance between $\{w_i\}$ and $\{d_i\}$, "$q_k$ is a scale factor indicating the importance of unit $k$ in the distance calculation. In most practical situations $q_k$ is set to 1." We assume that: a) $G(w_k/d_k)\ge 0$; b) $G(1)=0$; c) $G(\cdot)$ is differentiable; d) $G(\cdot)$ is strictly convex (i.e. the second derivative of $G$ is positive.)

The constraints are
$$
\sum_{k \in S} w_k \mathbf x_k = \mathbf t_{\mathbf x},
$$
where 
$$
\mathbf x_k = (x_{1k}, \ldots, x_{Jk}),
$$
and 
$$
\mathbf t_{\mathbf x}=(t_{x_1}, \ldots, t_{x_J}),
$$
with
$$
t_{x_j}=\sum_{k \in U} x_{jk}.
$$

**Example 1** Suppose that the population size $N$ is known. Setting 
$$
x_k =1\ \hbox{and}\ q_k=q\ \hbox{for}\ k\in S,
$$
we want to minimize
$$
f(w_k;\ k\in S)=\sum_{k \in S}d_k G(w_k/d_k),
$$
subject to
$$
\sum_{k \in S}w_k= N.
$$
It's easy to derive that
$$
w_k = N\frac{d_k}{\sum_{i \in S} d_i}.
$$
In this case, the calibrated total estimator is
\begin{align}
\hat{t}_{y,\ C} & = \sum_{k \in S}w_k y_k\\
&= \sum_{k \in S} N \frac{d_k}{\sum_{i \in S} d_i} y_k\\
&= N\frac{\hat{t}_{y,\ \pi}}{\hat{N}_{\pi}}.
\end{align}

**Example 2** Suppose $x_k$ is available for all $k \in S$ and it's known that $\sum_{k \in U} x_k= t_x$. Setting $q_k=x_k^{-1}$, we want to minimize
$$
f(w_k;\ k\in S)=\sum_{k\in S}d_k G(w_k/d_k) x_k,
$$
subject to
$$
\sum_{k\in S}w_k x_k=t_x.
$$
We can derive that
$$
w_k = d_k\frac{t_x}{\sum_{k \in S}d_k x_k}=d_k\frac{t_x}{\hat{t}_{x,\ \pi}}.
$$
In this case, the calibrated total estimator is
$$
\hat{t}_{y,\ C}=\frac{\hat{t}_{y,\ \pi}}{\hat{t}_{x,\pi }}t_x,
$$
which is the so-called *ratio estimator*.

**Example 3** Let $N_M$ and $N_F$ denote the numbers of males and females, respectively, in the population. Assume that $N_M$ and $N_F$ are known. Divide $S$ into $M$ and $F$, where $M$ is the set of males and $F$ is the set of females. We want to minimize
$$
f(w_k; k\in S) = \sum_{k \in M}d_k G(w_k/d_k)q_M + \sum_{k \in F}d_k G(w_k/d_k)q_F,
$$
subject to
$$
\sum_{k \in M}w_k= N_M\ \hbox{and}\ \sum_{k \in F}w_k=N_F.
$$
It's easy to show that
$$
w_k = \left\{
\begin{array}{cl}
d_k\frac{N_M}{\sum_{k \in M}d_k}, & \hbox{if}\ k\in M\\
d_k\frac{N_F}{\sum_{k \in F}d_k}, & \hbox{if}\ k\in F.
\end{array}
\right.
$$
In this case, the calibrated total estimator is
$$
\hat{t}_{y,\ c}=\frac{N_M}{\hat{N}_{M,\ \pi}}\hat{t}_{My,\ \pi} + \frac{N_F}{\hat{N}_{F,\ \pi}}\hat{t}_{Fy,\ \pi},
$$
where
\begin{align}
\hat{N}_{M,\ \pi}&=\sum_{k\in M} d_k,\\
\hat{N}_{F,\ \pi}&=\sum_{k\in F} d_k,\\
\hat{t}_{My,\ \pi}&=\sum_{k \in M}d_k y_k,\\
\hat{t}_{Fy,\ \pi}&=\sum_{k \in F}d_k y_k.
\end{align}
Notice that here we are talking about the so-called *post-stratified estimator*.



**Commonly used distance functions**



<!-- The output files for weighting system should comprise the following -->

<!-- - **weighting_system_main,** which is a table that contains -->
<!-- columns `unit_ID`, `selection_wgt`, `adj_wgt_4_nonresponse` (optional), `final_wgt` and `jk_rep_wgt_1`, ..., `jk_rep_wgt_100`. -->
<!-- - **benchmark tables**  -->

<!-- ## Selection weight -->

<!-- Selection weights are provided at the sampling design stage. The formula is: For unit $i$, its -->
<!-- $$ -->
<!-- \text{SelectionWgt}=\frac{1}{\text{inclusion probability of unit}\ i}. -->
<!-- $$ -->

<!-- ## Reweighting (or weighting adjustment) -->


# Sampling error




# (PART\*) Part II: Practice {-}

# Preliminary concepts and knowledge

## Concepts

## Knowledge about NZ



## Sampling design for 2018 TK

**Target population:** the people who are

- NZ usual residents living in private dwellings
- aged 15 or above
- Maori (ethnicity or descent)

**Sampling frame:** it's formed by using the 2018 Census data (i.e. individual table and dwelling table)

**Sampling scheme:** stratified two-stage sampling. In **Stage 1**, we take psus---SRS, and in **Stage 2**, we take individuals---SRS (firstly using stratification with age being the stratification variable). For each psu, we can know which *region* it belongs to, if it is *urban* or *rural*, if it has *high*/*low* Maori people, and based on the information we can partition all the psus into 50 strata (NB: this is after some necessary combination is done). 


**Sample sizes:** a) how many psus to be taken in each stratum; b) how many individuals to be taken from the selected psus.

**Notation:**

- $N:$ population size (or total number of ssus)
- $H:$ total number of strata
- $N_h:$ size of stratum $h$, for $h=1, 2,\ldots, H$
- $n:$ sample size
- $n_h:$ sample size of stratum $h$, for $h=1, 2,\ldots, H$
- $M:$ number of psus
- $M_h:$ number of psus in stratum $h$, for $h=1, 2,\ldots, H$ 
- $m_h:$ number of psus to be taken from stratum $h$, for $h=1, 2,\ldots, H$
- $n_{ih}:$ $n_h$ is split into $n_{1h}$, $n_{2h}$ and $n_3h$ in stage 2, where $i$ indicates age group.

**Step 1:** $n\longrightarrow \{n_1, n_2, \ldots, n_H\}$, for example
$$
n_h = \text{ceiling}\left(\frac{nN_h}{\sum_{h=1}^H N_h}\right).
$$

**Step 2:** find $m_h$, for example
$$
m_h = \text{ceiling}\left(\frac{n_h}{\frac{N_h}{M_h}}\right).
$$
Note that $\frac{N_h}{M_h}$ is average size of psus in stratum $h$.

**Step 3:** $n_h\longrightarrow \{n_{1h}, n_{2h}, n_{3h}\}$, for example
$$
n_{ih} = n_h\times f_i,
$$
where $f_i$ is *sampling fraction* for age group $i$ (e.g. $f_1=f_2=0.35,\ f_3=0.3$.)





# Software

# References {-}

**Haziza, D. and Beaumont, J.** (2017). Construction of Weights in Surveys: A Review. *Statistical Science*, Vol. 32, pp. 206-226.

**Till`r knitr::asis_output("\u00E9")`, Y. and Wilhelm, M.** (2017). Probability Sampling Design: Principles for Choice of Design and Balancing. *Statistical Science*, Vol. 32, pp. 176-189.

